---
layout: distill
title: "Crystal generative models: Inorganic Crystals and Metal-Orgranic Frameworks"
description: 
  In this work, we review to recent works on materials generation. One is SymmCD, which generates inorganic crystals by exploiting the special symmetry property of crystals called 'space groups'. The other is MOFDiff, a coarse-grained diffusion model that generates metal-organic frameworks (MOFs) by exploiting the modular nature of MOFs. 
date: 2025-06-01
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Nayoung Kim
    affiliations:
      name: KAIST


bibliography: 2025-04-28-analysing-the-spectral-biases-in-generative-models.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Motivation
    subsections:
      - name: Two Types of Crystals, Two Types of Challenges
  - name: "SymmCD: Symmetry-Preserving Crystal Generation with Diffusion Models"
    subsections:
      - name: "Preliminary: Crystal representation"
      - name: Asymmetric unit and site symmetries
      - name: "Pre-processing: Finding the Asymmetric Unit"
      - name: Training the model
      - name: Sampling and Post-processing
      - name: "Experimental results: what advantages does SymmCD have?"
  - name: "MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design"

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

<!-- preamble -->
<div class="preamble">
$$
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bbR}{\mathbb{R}}
$$
</div>

## Motivation

Materials span a wide range of structural and chemical complexities, from amorphous polymers to highly ordered solids. Among them, **crystals** refer to materials with a regular, repeating arrangement of atoms or molecules—a periodicity that defines their structure and often their properties. Many everyday materials are crystalline: *diamond*, known for its extreme hardness; *quartz*, used in electronics; and *table salt*, essential in daily life. Designing new crystalline materials with desirable properties is at the heart of breakthroughs in energy, electronics, and environmental sustainability.

The use of deep learning in **crystal generation** -- the automated design of novel crystal structures -- is a rapidly growing research area. Most existing work focuses on **inorganic crystals**, which include materials like semiconductors, superconductors, and catalysts. These materials play a central role in applications such as batteries, solar cells, and thermoelectrics.

A more recent and equally exciting focus has emerged around **metal-organic frameworks (MOFs)**. As the name suggests, MOFs are crystalline materials composed of metal clusters (or nodes) and organic linkers. Their porous structures and high tunability make them promising candidates for gas storage, catalysis, and especially carbon capture, including technologies like direct air capture (DAC) - a critical tool in the fight against climate change.

### Two Types of Crystals, Two Types of Challenges

While both inorganic crystals and MOFs are crystalline, their intrinsic differences pose distinct challenges for generative modeling.
- **Inorganic crystals** tend to have smaller unit cells and fewer atoms per structure. However, they *lack clear modular subunits*, making them harder to decompose or simplify. Each atom’s role and placement are tightly integrated within the overall crystal symmetry.
- **MOFs**, in contrast, can be enormous—sometimes with hundreds or even thousands of atoms per unit cell. Fortunately, they are highly modular, typically composed of well-defined building blocks: metal nodes and organic molecules. This *modularity* invites hierarchical or coarse-grained generation strategies.


|                   |     **Inorganic**       |       **MOF**        |
|-------------------|----------------------|--------------------------|
| Dataset | Materials Project | Core MOF, BW-DB | 
| Size    | ~10-50 atoms/unit cell | ~50-2000 atoms/unit cell|
| Key structural trait | symmetry-rich | modular |
| Generation challenge | symmetry | scale | 


In this blogpost, we explore two recent generative models that address these distinct challenges:
- **SymmCD (ICLR 2025)** targets *inorganic crystals* using a diffusion model with explicit symmetry considerations. By learning to generate only the *asymmetric unit* (the minimal building block of a crystal) and expanding it via symmetry operations, it ensures validity and efficiency in generation.
- **MOFDiff (ICLR 2024)** focuses on *metal-organic frameworks*, leveraging their modularity through a *coarse-grained representation*. Instead of generating atoms directly, MOFDiff learns to assemble metal and organic fragments into full MOF structures using a score-based generative model.

Here’s a quick comparison:

|                   |     **SymmCD**       |       **MOFDiff**        |
|-------------------|----------------------|--------------------------|
| Materials         | Inorganic crystals   | Metal-organic frameworks |
| Key idea    | Crystal symmetry -> asymmetric unit    | MOF modularity -> coarse-graining | 
| Method            | Diffusion            | Diffusion                |
| Model backbone    | Periodic GNN         | GemNet                   | 
| Published in      | ICLR 2025            | ICLR 2024                | 

The purpose of this blogpost is to provide a high-level overview of crystal generative models and how the property of each crystal could be utilized as an inductive bias in the model. It is assumed that many basics are known, such as molecule representation and diffusion models. 

## SymmCD: Symmetry-Preserving Crystal Generation with Diffusion Models

### Preliminary: Crystal representation

To represent a 3D structure of a **molecule** with $$N$$ atoms, we typically specify two things: the atom types $$\bA=(a_1, \dots, a_N) \in \cA^N$$, where each $$\cA$$ is the set of possible elements, and the 3D coordinates of those atoms,  $$\bX=(x_1, \dots, x_N) \in \bbR^{N \times 3}$$. 

Crystals are similar, but with one key addition, side, $$\bA$$ and $$\bX$$, we also need the lattice or the unit cell $$\mathbf{L}=(l_1, l_2, l_3) \in \bbR^{N \times 3}$$.  This compact representation $$\cM = (\bA, \bX, \bL)$$ captures all the necessary information about a crystal without explictly listing its infinitely repeating atoms. To recover the full, infinite structure, we simply have to translate the atoms along the lattice vectors $$l_1, l_2, l_3$$, we can recover the full, infinite crystal structure (Figure).

### Asymmetric unit and site symmetries

While the unit cell representation of a crystal $$\cM = (\bA, \bX, \bL)$$ is compact, SymmCD exploits an even more compact representation of a crystal called the **asymmetric unit**. The asymmetric unit can be replicated to construct the unit cell $$\cM$$, which can then be extended to infinite crystal structure afterward (Image). 

A helpful analogy to better understand the asymmetric unit is making a paper snowflake. Imagine folding a hexagonal sheet into a triangle, then cutting a small shape into it. When you unfold the paper, that shape appears multiple times, arranged symmetrically. In this analogy, the folded triangle represents the asymmetric unit, and unfolding it mimics the symmetry operations used to build a full crystal from its minimal part.

But here’s the subtle part: *where* you make the cut affects how many times it appears in the final pattern. A cut at the center might be repeated six times; one at the edge, only three. In crystallography, this concept is captured by the **site symmetry** -- it tells you how many times an atom is replicated based on its position.

SymmCD leverages these two simple concepts for inorganic crystal generation. Rather than generating the full crystal, it generates just the asymmetric unit, which contains the atom types, coordinates, lattice, and site symmetries. This simplifies the generation process while guaranteeing that the resulting structures are both symmetric and physically valid.

Mathematically, we represent an asymmetric unit with atom types $$\bA' = (a_1, \dots, A_M) \in \cA^M$$, 3D coordinates $$\bX=(x_1, \dots, x_M) \in \bbR^{M \times 3}$$, lattice as $$k \in \mathbb{R}^6$$, and the site symmetries of each atom $$\bS = (S_{x_1'}, \dots, S_{X_M'}) \in \mathcal{P}^M$$ where $$\mathcal{P}$$ denotes the set of all possible site symmetries. Note that $$M \leq N$$ (e.g., $$M=4.7$$ and $$N=18.9$$ on average for MP-20 dataset), since the asymmetric unit is a only a part of the unit cell. Notice that we can use a more compact 6-dimensional representation $$\mathbf{k} \in \bbR^6$$ to represent the lattice cell for a given asymmetric group. 

### Pre-processing: Finding the Asymmetric Unit

Before we can train a model to generate crystals, we first need to define what it should generate. Rather than modeling the full crystal, SymmCD focuses on its minimal building block: the **asymmetric unit**. But *how* do we find that?

It turns out that all crystals—yes, all of them—fall into just 230 possible space groups, each encoding a unique combination of symmetry operations. Given a crystal in unit cell form, $$\cM = (\bA, \bX, \bL)$$, we first identify its space group $$G$$ (there are python packages built for this purpose -- e.g., spglib). This space group tells us how the crystal repeats and, in turn, defines its asymmetric unit.

$$
\begin{equation}
\mathcal{D} = \{ (\bA, \bX, \bL)  \} \Rightarrow{} \mathcal{D} = \{ (G, k, \bA', \bX', \bS)  \}.
\end{equation}
$$

### Training the model

SymmCD models the joint distribution as:
$$
\begin{equation}
p_{\theta}(G, k, \bA', \bX', \bS)=p_{\theta}( k, \bA', \bX', \bS \vert G)p(G).
\end{equation}
$$

In other words, once a space group $$G$$ is specified (or sampled), the model learns to generate the corresponding asymmetric unit. To do this, SymmCD uses a diffusion model, where the atom types and site symmetries $$\bA', \bS$$ are learned with discrete diffusion and the lattice $$k, \bX'$$ are learned with continuous diffusion. 

### Sampling and Post-processing

Generating inorganic crystal structures with SymmCD is straightforward: (1) Choose a space group $$G$$ -- either randomly or specifying, (2) Sample $$(k, \bA', \bX', \bS) \sim p_{\theta}( k, \bA', \bX', \bS \vert G)$$ with the trained diffusion model, and (3) Project atoms into valid **Wyckoff positions** based on their site symmetries. 

Why is this projection needed? Recall the paper snowflake analogy: cuts made at different parts of the folded paper produce different numbers of copies. Similarly, certain regions of the asymmetric unit only permit specific site symmetries. If a generated atom-site symmetry pair is incompatible, we adjust (i.e., project) the atom’s position to a compatible one.

Finally, we replicate the asymmetric unit using its site symmetries to reconstruct the full crystal $$\cM = (\bA, \bX, \bL)$$. By shifting the focus from full crystals to asymmetric unit, SymmCD enjoys higher computational efficiency. 

### Experimental results: what advantages does SymmCD have?

In the experiments, we randomly generate 10,000 crystals in each baseline and identify the proportion of space group symmetries. The experiments show that both SymmCD and DiffCSP++ generates samples with *diverse* space group symmetries. Yet, SymmCD is able to generate more *novel* structures that are not in the training dataset. This is because DiffCSP++ starts with a template extracted from the training dataset and refine it with their model. 

## MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design

### Preliminary: A Scalable Representation for MOFs 

Metal–Organic Frameworks (MOFs) are some of the most structurally complex materials known. Compared to inorganic crystals, they’re much larger, often containing hundreds to thousands of atoms per unit cell. This scale makes direct generation at the atomic level both inefficient and error-prone for traditional generative models.

Fortunately, MOFs come with a built-in advantage: modularity. Each MOF is composed of repeating building blocks—metal clusters and organic linkers—that connect in well-defined ways. This modularity lets us step back and see MOFs not as clouds of atoms, but as assemblies of interacting components.

Instead of representing MOFs as full atomic structures $$(\bA, \bX, \bL)$$, MOFDiff uses a simplified, coarse-grained representation
$$
\begin{equation}
  \cM^C = (\bA^C, \bX^C, \bL)
\end{equation}
$$
where:
- $$\bA^C = (a_1^C, \dots, a_K^C) \in \mathbb{B}^{K}$$ are the building block types (e.g., a specific metal node or linker),
- $$\bX^C = (x_1^C, \dots x_k^C) \in \bbR^{K \times 3}$$ are their 3D Cartesian coordinates, and
- $$\bL$$ is the lattice that defines periodicity (same). 

Because the number of building blocks $$K$$ is often orders of magnitude smaller than the number of atoms $$N$$ (i.e., $$K \ll N$$), this representation is much more efficient to model. 

And that’s the core idea behind MOFDiff—instead of modeling the full atomic structure, it learns a score-based diffusion model over a simplified, coarse-grained representation of MOFs.

### Learning building block representations

But how do we represent these building blocks effectively?

A naive approach might be to extract all building blocks from the training dataset, assign each a one-hot ID, and then train a diffusion model: discrete diffusion for the building block types $$\bA^c$$, and continuous diffusions for their positions $$\bX^C$$ and lattice $$\bL$$. 

However, this naive strategy quickly falls apart. The training dataset contains millions of building blocks—around 2 million extracted from 289k MOFs—which makes one-hot encoding extremely sparse and hard to learn. Even worse, many of these building blocks are topologically identical—they have the same atom and bond structure (i.e., same 2D graph), differing only slightly in 3D geometry, making this representation very inefficient. 

To overcome this, MOFDiff learns a **dense**, **continuous** embedding for each building block that captures topological similarity while remaining efficient to model. It does this by training a **SE(3)-invariant graph neural network** takes as input the $$i$$ building block (i.e., its atom types and coordinates) and outputs a dense embedding $$\mathbf{b}_i \in \bbR^d$$ corresponding to that building block. 

To ensure that the learned embeddings group similar building blocks together, MOFDiff uses a **contrastive learning objective**. It first computes 2D fingerprints (ECFP4) to define molecular similarity, and then trains the model to bring structurally similar blocks close in embedding space.

The contrastive loss is defined as:

$$
\begin{equation}
   \mathcal{L}_C = - \log \sum_{i \in \mathbf{B}} \frac{\sum_{j \in \mathbf{B}_i^{+}} \exp(s_{i,j} / \tau)}{\frac{\sum_{j \in \mathbf{B}_i^{+}} \exp(s_{i,j} / \tau)}
\end{equation}
$$
where  
- $\mathbf{B}$ is a batch of building blocks,  
- $\mathbf{B}_i^{+} \subseteq \mathbf{B}$ are building blocks with the same ECFP4 fingerprint as $i$,  
- $s_{i,j}$ is the *cosine similarity* between projected building block embeddings:  
$$
\begin{equation}
   s_{i,j} = \frac{\mathbf{p}_i^{\top} \mathbf{p}_j}{\|\mathbf{p}_i\| \|\mathbf{p}_j\|}, \quad \mathbf{p}_i = \operatorname{MLP}(\mathbf{b}_i)
\end{equation}
$$
- $\tau$ is the temperature.

Once trained, this encoder maps each building block to a dense embedding. All MOFs in the dataset can then be transformed into their coarse-grained representation $$\cM = (\bA, \bX, \bL)$$, where $$\bA^C = (\mathbf{b}_1, \dots, \mathbf{b}_K) \in \bbR^{K \times d}$$ with $$d=32$$ -- a far more compact representation than a one-hot encoding over ~2M types of building blocks. 

### Training the model

With the coarse-grained representation in place, it’s time to train the generative model.

MOFDiff aims to model the full joint distribution over the coarse-grained structure and an auxiliary latent variable $$\mathbf{z}$$:
$$
\begin{equation}
  p_{\theta}(\bA^C, \bX^C, \bL, K, z) = p_{\theta}(L, K \vert \mathbf{z}) p_{\theta}(\bA^c, \bX^C \vert L, k, \mathbf{z}),
\end{equation}
$$
where:
- $$\bA^C=(\mathbf{b}_1, \dots, \mathbf{b}_K) \in \bbR^{K \times d}$$ and $$\bX^C = (x_1^C, \dots x_k^C) \in \bbR^{K \times 3}$$ are the building block types and positions with $$K$$ as the number of building blocks, 
- $$\bL$$ is the lattice,
- $$\mathbf{z}$$ is a latent vector used for generating the $$\bL$$, $$\bK$$.

This distribution is factorized as:
$$
\begin{equation}
  p_{\theta}(\bA^C, \bX^C, \bL, K, z) = p_{\theta}(L, K \vert \mathbf{z}) p_{\theta}(\bA^c, \bX^C \vert L, k, \mathbf{z}). 
\end{equation}
$$
Here, the first part, $$p_{\theta}(L, K \vert \mathbf{z})$$ is modeled using an MLP:
$$
\begin{equation}
  \hat{\bL}, \hat{K} = \operatorname{MLP}_{\bL, K}(mathbf{z}),
\end{equation}
$$
while the second part, $$p_{\theta}(\bA^c, \bX^C \vert L, k, \mathbf{z})$$ is modeled using score-based diffusion, where a periodic graph neural network denoiser $$\operatorname{PGNN}_D$$ predicts the noise in the atom types and coordinates as:
$$
\begin{equation}
  \mathbf{s}_{\bA^C}, \mathbf{s}_{\bX^C} = \operatorname{PGNN}_D(\tilde{\cM}_t^C, z),
\end{equation}
$$
where $$\tilde{\cM}_t^C=(\bA_t^C, \bX_t^C, \bL)$$ is the noised version of the coarse-grained structure at timestep $$t$$, and $$\mathbf{z}$$ is the latent vector produced with a periodict graph neural network encoder $$\operatorname{PGNN}_E$$: 
$$
\begin{equation}
  \mathbf{z} = \operatorname{PGNN}_E(\cM^C).
\end{equation}

where $$p_{\theta}(L, K \vert \mathbf{z})$$ simply learned as a degenerate distribution with $$\hat{\bL}, \hat{K} = \operatorname{MLP_{\bL, K}(mathbf{z})$$ and $$p_{\theta}(\bA^c, \bX^C \vert L, k, \mathbf{z})$$ is learned with a denoiser for score-based diffusion as: $$\mathbf{s}_{\bA^C}, \mathbf{s}_{\bX^C} = \operatorname{PGNN}_D(\c\tilde{M}_t^C, z)$$, where $$\operatorname{PGNN}_D$$ is a periodic graph-neural network (we omit $$K$$ since it is implictly included as the dimension). The additional latent representation $$\mathbf{z}$$ is learned with an encoder as $$\mathbf{z} = \operatorname{PGNN}_E(\cM^C)$$, where $$\operatorname{PGNN}_E$$ is a periodic graph neural network encoder as well. 

#### Objective function
The denoiser $$\operatorname{PGNN}_D(\cM_t^C, z)$$ is trained using the standard denoising score objective:
$$
\begin{equation}
\mathcal{L}_{\boldsymbol{A}}=\mathbb{E}_{t, \boldsymbol{M}^C, \boldsymbol{\epsilon}_{\boldsymbol{A}}}\left[\left\|\boldsymbol{\epsilon}_{\boldsymbol{A}}-\boldsymbol{s}_{\boldsymbol{A}_t^C, \boldsymbol{z}}\right\|^2\right], \quad
\mathcal{L}_{\boldsymbol{X}}=\mathbb{E}_{t, \boldsymbol{M}^C, \boldsymbol{\epsilon}_{\boldsymbol{X}}}\left[\sigma_t^2\left\|\boldsymbol{\epsilon}_{\boldsymbol{X}}-\boldsymbol{s}_{\boldsymbol{X}_t^C, \boldsymbol{z}}\right\|^2\right].
\end{equation}
$$
$$\operatorname{MLP_{\bL, K}(mathbf{z})$$ is supervised with the measn squared error and cross-entropy losses:
$$
\begin{equation}
  \mathcal{L}_{\boldsymbol{L}, K}=\|\boldsymbol{L}-\hat{\boldsymbol{L}}\|^2+\operatorname{CrossEntropy}(K, \hat{K}).
\end{equation}
$$
Finally, the encoder $$\operatorname{PGNN}_E$$ is regularized with a KL divergence $$\mathcal{L}_{\mathrm{KL}}$$ to match a standard normal prior. 

The overall training objective for MOFDiff combines all components as:
$$
\begin{equation}
\mathcal{L}_{\text {MOFDiff }}=\mathcal{L}_{\boldsymbol{A}}+\mathcal{L}_{\boldsymbol{X}}+\mathcal{L}_{\boldsymbol{L}, K}+\beta_{\mathrm{KL}} \mathcal{L}_{\mathrm{KL}}.
\end{equation}
$$
where $$\beta_{\mathrm{KL}}$$ is a hyperparameter that controls the strength of KL regularization, set to 0.01. 

### Sampling and post-processing

Once MOFDiff is trained, we can generate a new MOF structure with the following process:
1. **Sample** a random latent code $$\mathbf{z} \sim \mathcal{N}(0,I)$$.
2. **Predict** the lattice and number of building blocks:
$$
\begin{equation}
  \hat{\bL}, \hat{K} = \operatorname{MLP}_{\bL, K}(mathbf{z})
\end{equation}
$$
3. **Run denoising diffusion** with the trained denoiser $$\operatorname{PGNN}_D(\tilde{\cM}_t^C, z)$$ to generate the coarse-grained structure $$(\bA^C, \bX^C, \bmL)$$. 

At this point, we have a **coarse-grained MOF**. We now need to recover the full atomic structure with fine-graned details. Here's how:
1. **Building block decoding**. For each generated embedding $$\bA^C$$, we retrieve the building block from the training dataset with the closest embedding with nearest neighbor search. This gives us the actual atom types and coordinates of the building block. 
2. **Self-assembly (orientation prediction)**. Since we have only predicted the center of mass of each building block with $$\bX^C$$, we now need to know *how to orient them*. MOFDiff uses an optimization-based **sef-assembly algorithm** to find the rotation that maximizes the alignment of connection points between the building blocks. This maximizes the connectivity and minimizes the gaps between the metal nodes and organic linkers. 
3. **Force field relaxatiion**. Finally, the assembled structure undergoes relaxation with energy minimziation with the UFF force field. This step ensures that we can make a final fine-grained refinement to get a physical structure. 

### Experimental results: MOFDiff can generate valid, novel, unique structures.

How does MOFDiff perform in practice? To evaluate it, the authors sampled 10,000 MOF structures and assessed them on three key metrics: **validity**, **novelty**, and **uniqueness**.
- **Validity**: A structure is considered valid if it passes the check with a python package called `MOFchecker`, which assess whether a MOF is chemically and physicall valid based on a set of criteria including the presence of at least one metal, carbon, and hydrogen atom, overlapping atoms, and valency.  
- **Novelty**: A structure is novel if it nodes not exist in the training dataset. This is measured with `MOFid`, which computes a unique identifier for a MOF baesd on its building blocks and connectivity.
- **Uniqueness**: We find the unique structures by filtering out the duplicates from the generated set of structures.

Out of 10,000 generated structures, **3,012 passed the validity check** and **2,998** were **valid**, **novely**, and **unique**. That's nearly 3,000 high-quality MOFs generated from scratch -- an impressive result given the structural complexity and the size of MOFs. 

### Conditional generation: designing MOFs for carbon capture

Thanks to the latent variable $$\mathbf{z}$$, MOFDiff can also be used for property-guided generation.

Suppose you want to discover MOFs with high CO2 working capacity -- a key metric for carbon capture technology. MOFDiff enables this by learning a simple property predictor during training:
$$
\begin{equation}
  \hat{\mathbf{c}} = \operatorname{MLP}_P(\mathbf{z}),
\end{equation}
$$
where $$\mathbf{c}$$ is the property of interest (e.g., CO2 capacity). The predictor is trained with the mean squared error loss using known property labels. 

Then, during sampling, MOFDiff generates many latent codes $$\mathbf{z} \sim \mathcal{N}(0, I)$$, computes the CO2 working capacity $$\hat{\mathbf{c}}$$, and filters for those with high CO@ working capacity. 

In experiments, this approach successfully generates MOFs with higher CO2 working capcity than those in the training dataset, demonstrating MOFDiff's potential for targeted material discovery. 
